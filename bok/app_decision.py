import os
import re
import json
import boto3
from boto3.dynamodb.conditions import Key
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta

from pdfminer.high_level import extract_text
from openai import OpenAI

# --------------------------
# 0. AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# --------------------------
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table(os.environ["DDB_TABLE"])

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])


# ------------------------
# 1. ì •ê·œí™” í•¨ìˆ˜
# ------------------------
def normalize_text(text: str) -> str:
    text = text.replace("Å¾", "Â·")
    lines = text.splitlines()
    cleaned_lines = []

    line_remove_patterns = [
        r"^\s*\[?\s*(ê·¸ë¦¼|í‘œ)\s*\d+",
        r"^\s*ì£¼\s*[: ]?\s*\d+",
        r"^\s*(ìžë£Œ|ì¶œì²˜)\s*[: ]?",
        r"^\s*-\s*\d+\s*-",
    ]

    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if any(re.search(p, stripped) for p in line_remove_patterns):
            continue
        cleaned_lines.append(stripped)

    text = " ".join(cleaned_lines)
    text = re.sub(r"\d+\)", " ", text)
    text = re.sub(r"[^\w\s.,!?ê°€-íž£()/%\-â… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]", " ", text)
    text = re.sub(r"\s+", " ", text)

    return text.strip()


def unify_roman_and_symbols(text: str) -> str:
    variants_I = [
        "â… ", "ï¼©", "ð‘°", "ðˆ", "ð˜", "ð•€", "ð–¨", "ð—œ", "ð›ª",
    ]
    for v in variants_I:
        text = text.replace(v, "I")

    for v in ["â€“", "â€”", "âˆ’", "ï¹£", "â€"]:
        text = text.replace(v, "-")

    for v in ["ã€‚", "ï¼Ž", "ï½¡"]:
        text = text.replace(v, ".")

    return text


def remove_table_of_contents(text: str) -> str:
    m = re.search(r"[â… I]\s*-\s*1", text)
    if m:
        return text[m.start():]
    return text


def cut_statistics_section(text: str) -> str:
    m = re.search(r"ì£¼ìš”\s*í†µê³„\s*ë°\s*ì°¸ê³ ", text)
    if m:
        return text[:m.start()].strip()
    return text


def clean_non_text_blocks(paragraph: str) -> str:
    sents = re.split(r"(?<=[.!?])\s+", paragraph)
    cleaned = []

    for s in sents:
        s = s.strip()
        if not s:
            continue

        if re.search(r"^(ê·¸ë¦¼|í‘œ)\s*\d+", s):
            continue
        if re.search(r"^ì£¼\s*\d+", s):
            continue
        if re.search(r"^(ìžë£Œ|ì¶œì²˜)[: ]", s):
            continue

        tokens = s.split()
        num_tokens = sum(1 for t in tokens if re.match(r"^[\d\.\-,/]+$", t))
        if len(tokens) > 0 and num_tokens / len(tokens) > 0.6:
            continue

        cleaned.append(s)

    return " ".join(cleaned).strip()


def extract_paragraphs(raw_text: str) -> list[str]:
    # (1) normalize
    text = unify_roman_and_symbols(raw_text)
    text = normalize_text(text)
    # (2) ëª©ì°¨ ì œê±°
    text = remove_table_of_contents(text)
    # (3) ë§ë¯¸ í†µê³„ ì œê±°
    text = cut_statistics_section(text)
    # (4) ì¤„ ê¸°ë°˜ ë¬¸ë‹¨ í›„ë³´
    cleaned = clean_non_text_blocks(text)
    return cleaned

# ------------------------
# 2. summary prompt & batch jsonl
# ------------------------
def estimate_tokens(text: str) -> int:
    return int(len(text) / 3)


def decide_summary_lines(tokens: int) -> str:
    if tokens < 1000:
        return "3~4ì¤„"
    elif tokens < 2000:
        return "5~6ì¤„"
    elif tokens < 3000:
        return "6~8ì¤„"
    return "8~10ì¤„"


def build_system_prompt(text: str):
    tokens = estimate_tokens(text)
    lines = decide_summary_lines(tokens)
    return f"""
ë„ˆëŠ” ê²½ì œ ë¶„ì„ ì „ë¬¸ê°€ì•¼. 
ë‹¤ìŒ ë¬¸ë‹¨ì„ ë¶ˆí•„ìš”í•œ ë¬¸ìž¥ì´ë‚˜ ë°˜ë³µ í‘œí˜„ì€ ì œê±°í•˜ê³  í•µì‹¬ ê²½ì œ íë¦„ë§Œ ì •ë¦¬í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ {lines}ë¡œ ìš”ì•½í•´ì¤˜. 
ìš”ì•½ì€ 'í•µì‹¬ ë¬¸ìž¥ ë‹¨ìœ„ ë¬¸ìžì—´ ë¦¬ìŠ¤íŠ¸' í˜•íƒœë¡œ ë°˜í™˜í•´ì•¼ í•´.
ë˜í•œ ê¸°ì¤€ ê¸ˆë¦¬ ë³€ë™ ì—¬ë¶€ë¥¼ ì œëª©ìœ¼ë¡œ 1ì¤„ë¡œ ìž‘ì„±í•´ì¤˜.
"""

def create_batch_jsonl(paragraph, output_file="/tmp/batch_input.jsonl"):
    with open(output_file, "w", encoding="utf-8") as f:
        prompt = build_system_prompt(paragraph)
        item = {
            "custom_id": f"para-0000",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-5.1",
                "messages": [
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": paragraph},
                ],
                "max_completion_tokens": 800,
                "temperature": 0,
                "response_format": {
                    "type": "json_schema",
                    "json_schema": {
                      "name": "paragraph_summary",
                      "schema": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string"
                            },
                            "summary": {
                                "type": "array",
                                "items": {
                                    "type": "string"
                                }
                            }
                        },
                        "required": ["title", "summary"],
                      }
                    }
                  },
                },
            }
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

    return output_file

# ------------------------
# 3. PDF ë‹¤ìš´ë¡œë“œ & Batch ì œì¶œ
# ------------------------
PUBLISH_DATES = [
    (1, 16),
    (2, 25),
    (4, 17),
    (5, 29),
    (7, 10),
    (8, 28),
    (10, 23),
    (11, 27),
]


def get_target_report_month(today=None):
    if today is None:
        today = datetime.today()

    year = today.year
    release_days = []

    for (m, d) in PUBLISH_DATES:
        release_day = datetime(year, m, d)
        available = release_day + timedelta(days=0)
        release_days.append((m, available))

    release_days.sort(key=lambda x: x[1])

    target = None
    for (m, available) in release_days:
        if today >= available:
            target = m
        else:
            break
    return target


def extract_pdf_links(page_url):
    soup = BeautifulSoup(requests.get(page_url).text, "html.parser")
    rows = soup.select("table#tableId tbody tr")

    pdf_links = []
    for row in rows:
        tds = row.find_all("td")
        if not tds:
            continue
        last_td = tds[1]
        link = last_td.select_one("div.fileGoupBox ul li:nth-of-type(2) a.i-download[href]")
        if not link:
            continue

        filename = link.get_text(strip=True)
        pdf_url = urljoin(page_url, link["href"])
        pdf_links.append({"filename": filename, "url": pdf_url})

    return pdf_links


def should_download_today(page_url, today=None):
    month = get_target_report_month(today)
    if month is None:
        return None

    pdfs = extract_pdf_links(page_url)
    year = today.year

    short_code = f"{year % 100:02d}{month:02d}"
    expected_code = f"{year}-{month:02d}"

    for info in pdfs:
        if short_code in info["filename"]:
            return {
                **info,
                "code": expected_code
            }

    return None

def download_pdf(pdf_url, filename):
    path = f"/tmp/{filename}"
    r = requests.get(pdf_url)
    r.raise_for_status()
    with open(path, "wb") as f:
        f.write(r.content)
    return path


def submit_batch(file_path):
    batch_input = client.files.create(
        file=open(file_path, "rb"),
        purpose="batch",
    )

    batch_job = client.batches.create(
        input_file_id=batch_input.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
    )
    return batch_job


def save_batch_id(batch_id: str, code: str, type_: str):
    item = {
        "code_type": f"{code}#{type_}",
        "code": code,
        "type": type_,
        "batch_id": batch_id,
        "status": "pending",
        "created_at": datetime.now().isoformat(),
    }

    table.put_item(Item=item)
    return item

# ------------------------
# 4. Slack ë©”ì‹œì§€
# ------------------------
def send_slack_message(text: str):
    webhook = os.environ["SLACK_WEBHOOK_URL"]
    payload = {"text": text}

    resp = requests.post(
        webhook,
        headers={"Content-Type": "application/json"},
        data=json.dumps(payload),
    )
    print("[Slack] status:", resp.status_code)
    print("[Slack] response:", resp.text)

def exists_batch(code, type_):
    code_type = f"{code}#{type_}"

    resp = table.query(
        KeyConditionExpression=Key("code_type").eq(code_type)
    )

    return resp["Count"] > 0

# ------------------------
# 5. Lambda Handler
# ------------------------
def lambda_handler(event, context):
    BOK_PAGE_URL = os.environ["BOK_PAGE_URL"]

    pdf_info = should_download_today(BOK_PAGE_URL, today=datetime.today())
    if not pdf_info:
        return {"message": "No PDF available today."}
    if exists_batch(pdf_info["code"], "bok-decision"):
        return {"message": "Already process."}
    pdf_path = download_pdf(pdf_info["url"], pdf_info["filename"])

    raw = extract_text(pdf_path)
    paragraphs = extract_paragraphs(raw)
    jsonl_path = create_batch_jsonl(paragraphs)

    batch = submit_batch(jsonl_path)
    save_batch_id(batch.id, pdf_info["code"], "bok-decision")

    msg = f"ðŸ“Œ *OpenAI Batch Decision ìš”ì²­ ì™„ë£Œ!*\nâ€¢ Batch ID: `{batch.id}`"
    send_slack_message(msg)

    return {
        "status": "submitted",
        "batch_id": batch.id,
        "paragraphs": len(paragraphs),
    }
