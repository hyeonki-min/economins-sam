from common.slack import send_slack_message
import os
import re
import json
import boto3
from boto3.dynamodb.conditions import Key
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta

from pdfminer.high_level import extract_text
from openai import OpenAI

# --------------------------
# 0. AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# --------------------------
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table(os.environ["DDB_TABLE"])

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
BOK_PAGE_URL = os.environ["BOK_PAGE_URL"]


# ------------------------
# 1. ì •ê·œí™” í•¨ìˆ˜
# ------------------------
def normalize_text(text: str) -> str:
    text = text.replace("Å¾", "Â·")
    lines = text.splitlines()
    cleaned_lines = []

    line_remove_patterns = [
        r"^\s*\[?\s*(ê·¸ë¦¼|í‘œ)\s*\d+",
        r"^\s*ì£¼\s*[: ]?\s*\d+",
        r"^\s*(ìžë£Œ|ì¶œì²˜)\s*[: ]?",
        r"^\s*-\s*\d+\s*-",
    ]

    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if any(re.search(p, stripped) for p in line_remove_patterns):
            continue
        cleaned_lines.append(stripped)

    text = " ".join(cleaned_lines)
    text = re.sub(r"\d+\)", " ", text)
    text = re.sub(r"[^\w\s.,!?ê°€-íž£()/%\-â… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]", " ", text)
    text = re.sub(r"\s+", " ", text)

    return text.strip()


def unify_roman_and_symbols(text: str) -> str:
    variants_I = [
        "â… ", "ï¼©", "ð‘°", "ðˆ", "ð˜", "ð•€", "ð–¨", "ð—œ", "ð›ª",
    ]
    for v in variants_I:
        text = text.replace(v, "I")

    for v in ["â€“", "â€”", "âˆ’", "ï¹£", "â€"]:
        text = text.replace(v, "-")

    for v in ["ã€‚", "ï¼Ž", "ï½¡"]:
        text = text.replace(v, ".")

    return text


def remove_table_of_contents(text: str) -> str:
    m = re.search(r"[â… I]\s*-\s*1", text)
    if m:
        return text[m.start():]
    return text


def cut_statistics_section(text: str) -> str:
    m = re.search(r"ì£¼ìš”\s*í†µê³„\s*ë°\s*ì°¸ê³ ", text)
    if m:
        return text[:m.start()].strip()
    return text


def clean_non_text_blocks(paragraph: str) -> str:
    sents = re.split(r"(?<=[.!?])\s+", paragraph)
    cleaned = []

    for s in sents:
        s = s.strip()
        if not s:
            continue

        if re.search(r"^(ê·¸ë¦¼|í‘œ)\s*\d+", s):
            continue
        if re.search(r"^ì£¼\s*\d+", s):
            continue
        if re.search(r"^(ìžë£Œ|ì¶œì²˜)[: ]", s):
            continue

        tokens = s.split()
        num_tokens = sum(1 for t in tokens if re.match(r"^[\d\.\-,/]+$", t))
        if len(tokens) > 0 and num_tokens / len(tokens) > 0.6:
            continue

        cleaned.append(s)

    return " ".join(cleaned).strip()


def extract_paragraphs(raw_text: str) -> list[str]:
    # (1) normalize
    text = unify_roman_and_symbols(raw_text)
    text = normalize_text(text)
    # (2) ëª©ì°¨ ì œê±°
    text = remove_table_of_contents(text)
    # (3) ë§ë¯¸ í†µê³„ ì œê±°
    text = cut_statistics_section(text)
    # (4) ì¤„ ê¸°ë°˜ ë¬¸ë‹¨ í›„ë³´
    cleaned = clean_non_text_blocks(text)
    return cleaned

# ------------------------
# 2. summary prompt & batch jsonl
# ------------------------
def estimate_tokens(text: str) -> int:
    return int(len(text) / 3)


def decide_summary_lines(tokens: int) -> str:
    if tokens < 1000:
        return "3~4ì¤„"
    elif tokens < 2000:
        return "5~6ì¤„"
    elif tokens < 3000:
        return "6~8ì¤„"
    return "8~10ì¤„"


def build_system_prompt(text: str):
    tokens = estimate_tokens(text)
    lines = decide_summary_lines(tokens)
    return f"""
ë„ˆëŠ” ê²½ì œ ë¶„ì„ ì „ë¬¸ê°€ì•¼. 
ë‹¤ìŒ ë¬¸ë‹¨ì„ ë¶ˆí•„ìš”í•œ ë¬¸ìž¥ì´ë‚˜ ë°˜ë³µ í‘œí˜„ì€ ì œê±°í•˜ê³  í•µì‹¬ ê²½ì œ íë¦„ë§Œ ì •ë¦¬í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ {lines}ë¡œ ìš”ì•½í•´ì¤˜. 
ìš”ì•½ì€ 'í•µì‹¬ ë¬¸ìž¥ ë‹¨ìœ„ ë¬¸ìžì—´ ë¦¬ìŠ¤íŠ¸' í˜•íƒœë¡œ ë°˜í™˜í•´ì•¼ í•´.
ë˜í•œ ê¸°ì¤€ ê¸ˆë¦¬ ë³€ë™ ì—¬ë¶€ë¥¼ ì œëª©ìœ¼ë¡œ 1ì¤„ë¡œ ìž‘ì„±í•´ì¤˜.
"""

def create_batch_jsonl(paragraph, output_file="/tmp/batch_input.jsonl"):
    with open(output_file, "w", encoding="utf-8") as f:
        prompt = build_system_prompt(paragraph)
        item = {
            "custom_id": f"para-0000",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-5.2",
                "messages": [
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": paragraph},
                ],
                "max_completion_tokens": 800,
                "temperature": 0,
                "response_format": {
                    "type": "json_schema",
                    "json_schema": {
                      "name": "paragraph_summary",
                      "schema": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string"
                            },
                            "summary": {
                                "type": "array",
                                "items": {
                                    "type": "string"
                                }
                            }
                        },
                        "required": ["title", "summary"],
                      }
                    }
                  },
                },
            }
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

    return output_file


PUBLISH_DATES = [
    (1, 16),
    (2, 25),
    (4, 17),
    (5, 29),
    (7, 10),
    (8, 28),
    (10, 23),
    (11, 27),
]


def get_target_report_month(today=None):
    today = today or datetime.today()
    year = today.year

    release_days = [
        (m, datetime(year, m, d)) for m, d in PUBLISH_DATES
    ]
    release_days.sort(key=lambda x: x[1])

    target = None
    for m, available in release_days:
        if today >= available:
            target = m
        else:
            break
    return target


def extract_pdf_links(page_url):
    soup = BeautifulSoup(requests.get(page_url).text, "html.parser")
    rows = soup.select("table#tableId tbody tr")

    pdf_links = []
    for row in rows:
        tds = row.find_all("td")
        if not tds:
            continue

        link = tds[1].select_one(
            "div.fileGoupBox ul li:nth-of-type(2) a.i-download[href]"
        )
        if not link:
            continue

        pdf_links.append({
            "filename": link.get_text(strip=True),
            "url": urljoin(page_url, link["href"]),
        })
    return pdf_links


def should_download_today(today=None):
    today = today or datetime.today()
    month = get_target_report_month(today)
    if not month:
        return None

    pdfs = extract_pdf_links(BOK_PAGE_URL)
    year = today.year
    short_code = f"{year % 100:02d}{month:02d}"
    expected_code = f"{year}-{month:02d}"

    for info in pdfs:
        if short_code in info["filename"]:
            return {**info, "code": expected_code}

    return None


def download_pdf(pdf_url, filename):
    path = f"/tmp/{filename}"
    r = requests.get(pdf_url, timeout=10)
    r.raise_for_status()
    with open(path, "wb") as f:
        f.write(r.content)
    return path


def submit_batch(file_path):
    batch_input = client.files.create(
        file=open(file_path, "rb"),
        purpose="batch",
    )
    return client.batches.create(
        input_file_id=batch_input.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
    )


def exists_batch(code, type_):
    resp = table.query(
        KeyConditionExpression=Key("code_type").eq(f"{code}#{type_}")
    )
    return resp["Count"] > 0


def save_batch(batch_id, code, type_):
    table.put_item(
        Item={
            "code_type": f"{code}#{type_}",
            "code": code,
            "type": type_,
            "batch_id": batch_id,
            "status": "pending",
            "created_at": datetime.utcnow().isoformat(),
        }
    )


def run():
    pdf_info = should_download_today()

    if not pdf_info:
        return {
            "status": "NO_DATA",
            "reason": "no pdf today",
        }

    if exists_batch(pdf_info["code"], "bok-decision"):
        return {
            "status": "NO_DATA",
            "reason": "already processed",
            "code": pdf_info["code"],
        }

    pdf_path = download_pdf(pdf_info["url"], pdf_info["filename"])

    raw_text = extract_text(pdf_path)
    paragraph = extract_paragraphs(raw_text)

    jsonl_path = create_batch_jsonl(paragraph)
    batch = submit_batch(jsonl_path)

    save_batch(batch.id, pdf_info["code"], "bok-decision")

    return {
        "status": "SUCCESS",
        "batch_id": batch.id,
        "code": pdf_info["code"],
        "paragraphs": len(paragraph),
    }


def lambda_handler(event, context):
    try:
        result = run()

        send_slack_message(
            service="BOK | Decision Batch",
            message=json.dumps(result, ensure_ascii=False),
            status=result["status"],
        )

        return {
            "statusCode": 200,
            "body": json.dumps(result, ensure_ascii=False),
        }

    except Exception as e:
        send_slack_message(
            service="BOK | Decision Batch",
            message=str(e),
            status="ERROR",
        )
        raise
